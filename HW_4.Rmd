---
title: "HW_4"
author: "Advait Phadke"
date: "2025-02-15"
output: pdf_document
---

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(ggplot2)
library(tidyverse)
library(tibble)
library(knitr)
library(mosaic)
```

Advait Phadke, UT EID: amp7984

Github Repo:

## Problem 1

```{r}

simulated_trades = do(100000)*nflip(n=2021, prob=0.024)

#head(simulated_trades) 

ggplot(simulated_trades) + 
  geom_histogram(aes(x = nflip), fill = "lightblue4", color = "ivory", binwidth = 1) + 
  labs(x = "Flagged Trades", y = "Counts")

#sum(simulated_trades$nflip >= 70)

```

The null hypothesis is that the traders in Iron Bank have not done anything illegal violating insider trading laws, and their trades have just been flagged by the SEC because of their random flagging that occurs at a rate of 2.4% for even trades that are legal.

The test statistic is the 70 flagged trades out of the 2021 trades (proportion - 0.0346) taken by Iron Bank traders.

The histogram shows the number of trades that were flagged for each simulation of 2021 trades with a 2.4% probability of being flagged, all for 100,000 simulations of 2021 trades. Out of all of these trades, 212 had 70 or more trades flagged, giving a p-value of 0.00212.

Given the low p-value of 0.00212, it seems like the null hypothesis is not true and that Iron Bank Traders are violating insider trading laws. With a 0.212% chance of having 70 or more trades flagged out of 2021 according to the simulation, this seems very unlikely to happen based on the random flagging done by the SEC. There is obviously still a slight chance that it could have happened due to chance, but it's hard to believe that.

## Problem 2

```{r}

simulated_inspections = do(100000) * nflip(n = 50, prob = 0.03)

#head(simulated_inspections)

ggplot(simulated_inspections) + 
  geom_histogram(aes(x = nflip), fill = "steelblue", color = "ivory", binwidth = 1) + 
  labs(x = "Violations Out of 50 Inspections", y = "Counts")

#sum(simulated_inspections$nflip >= 8)


```

The null hypothesis is that the restaurant Gourmet Bites hasn't actually violated health codes, and their locations with health code violations reported have just been due to the 3% chance of health code violations being reported for any restaurant due to random factors and chance.

The test statistic is the 8 locations out of the 50 locations inspected that violated a health code. (proportion - 0.16)

The histogram shows the number of locations that were flagged for each simulation of 50 locations with a 3% probability of being flagged, all for 100,000 simulations of 50 inspections. Out of all of these inspections, 15 had 8 or more locations flagged, giving a p-value of 0.00015.

Given the extremely low p-value of 0.00015, it seems like the null hypothesis is not true and that Gourmet Bites truly has violated health codes. With a 0.015% chance of having 8 or more locations flagged out of 50 according to the simulation, this seems very unlikely to happen based on the flagging done by the inspectors due to random factors and chance.

## Problem 3

```{r}

expected_distribution = c(Group1 = 0.3, Group2 = 0.25, Group3 = 0.20, Group4 = 0.15, Group5 = 0.10)

observed_counts = c(Group1 = 85, Group2 = 56, Group3 = 59, Group4 = 17, Group5 = 13)

sum(observed_counts)

chi_squared_stat = function(observed, expected) {
  sum((observed - expected)^2 / expected)
}

actual_chi2 = chi_squared_stat(observed_counts, 230*expected_distribution)


sim_chi2 = do(100000)*{
  simulated_counts = rmultinom(1, 230, expected_distribution)
  this_chi2 = chi_squared_stat(simulated_counts, 230*expected_distribution)
  c(chi2 = this_chi2) # return a vector with names and values
}

sum(sim_chi2$chi2 >= actual_chi2)


ggplot(sim_chi2) + 
  geom_histogram(aes(x=chi2), fill = "darkseagreen", color = "ivory", binwidth = 0.75) + 
  labs(x = "Chi-Squared Stats of Simulated Jury", y = "Counts")
```

Null Hypothesis: There is no racial bias involved in the jury selection process, and the discrepancy between the eligible jury population and the jury in regards to the percentage of each racial group is due to random chance.

The test statistic is a certain jury that was selected with the following number of people from each group: Group1 - 85, Group2 - 56, Group3 - 59, Group4 - 27, Group5 - 13

After doing multinomial sampling and simulating a random selection of 100,000 juries with the given probability of choosing a person from each group, we can see a histogram of the chi-squared values for all of these simulated juries. The chi-squared value of the jury in question is 20.648. Out of the 100,000 simulated juries, only 55 had a chi-squared value that high or higher. This gives a p-value of 0.00055.

This is an extremely low p-value, which suggests a systematic bias in the process of selecting people for the jury. Now, there is always a chance this happened by chance, but that seems very unlikely in this scenario. Another explanation might be that more people are removed for cause or for peremptory challenges from certain groups compared to others, changing the percentage of each group in the actual eligible jury pool. We could investigate further by getting data on a lot of juries and calculating the chi-squared values for those juries and comparing the jury in question's chi-squared value to those. This could support the argument made earlier, indicating that it isn't due to a racial bias, however it could also suggest that there is racial bias in the process of selecting all juries. This would require even further investigation and a lot of data.

## Problem 4

```{r}

#Part A

raw_brown_sentences = readLines("brown_sentences.txt")

letter_frequencies = read.csv("letter_frequencies.csv")

calculate_chi_squared = function(sentence, freq_table) {
  
  # Ensure letter frequencies are normalized and sum to 1
  freq_table$Probability = freq_table$Probability / sum(freq_table$Probability)
  
  # Remove non-letters and convert to uppercase
  clean_sentence = gsub("[^A-Za-z]", "", sentence)
  clean_sentence = toupper(clean_sentence)
  
  # Count the occurrences of each letter in the sentence
  observed_counts = table(factor(strsplit(clean_sentence, "")[[1]], levels = freq_table$Letter))
  
  # Calculate expected counts
  total_letters = sum(observed_counts)
  expected_counts = total_letters * freq_table$Probability
  
  # Chi-squared statistic
  chi_squared_stat = sum((observed_counts - expected_counts)^2 / expected_counts)
  
  return(chi_squared_stat)
}

chi_squared_values <- vector()

# Loop through each sentence in raw_brown_sentences
for (sentence in raw_brown_sentences) {
  # Calculate the chi-squared value for the sentence
  chi_squared_value <- calculate_chi_squared(sentence, letter_frequencies)
  
  # Add the chi-squared value to the list
  chi_squared_values <- c(chi_squared_values, chi_squared_value)
}

# Turn into a dataframe 
chi_squared_df <- data.frame(chi_squared_values)

#Plot Histogram of null/reference distribution
ggplot(chi_squared_df, aes(x = chi_squared_values)) + 
  geom_histogram(binwidth = 2.5, fill = "darkkhaki", color = "ivory") +
  labs(x = "Chi-Squared Value", y = "Counts")


#Part B

sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations."
)

sentence_number = c()
p_value = c()
p_value_df <- data.frame(sentence_number, p_value)
  
count = 1

for (sentence in sentences) {
  
  # Calculate the chi-squared value for the current sentence
  chi = calculate_chi_squared(sentence, letter_frequencies)
  
  # Calculate the p-value
  p_value_result <- sum(chi_squared_values >= chi) / length(chi_squared_values)

  # Round the p-value result to 3 decimal places
  p_value_result_rounded <- round(p_value_result, 3)
  
  # Append the sentence number and rounded p-value to the data frame
  p_value_df <- rbind(p_value_df, data.frame(sentence_number = count, p_value = p_value_result_rounded))
  
  # Increase sentence count by 1
  count = count + 1
}

p_value_table = kable(p_value_df)

print(p_value_table)


```
